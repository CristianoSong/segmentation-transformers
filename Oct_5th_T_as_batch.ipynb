{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### T-frames as mini batch\r\n",
    "Try to consider T-frames as a mini batch, which means return ground truth for each frame.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import torch\r\n",
    "import random\r\n",
    "import cv2\r\n",
    "import math\r\n",
    "import numpy as np\r\n",
    "import torchvision\r\n",
    "from pathlib import Path\r\n",
    "from torch import nn\r\n",
    "from torch import optim\r\n",
    "import torch.utils.data as Data\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "from torch.nn import functional as F\r\n",
    "from torchvision import models, transforms\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import matplotlib\r\n",
    "import tqdm\r\n",
    "from sklearn.metrics import jaccard_score\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_path = Path(\"C:/Users/Siyao/Downloads/EndoVis2017Data\")\r\n",
    "train_path = data_path / \"cropped_train\"\r\n",
    "\r\n",
    "def get_split(fold):\r\n",
    "    \"\"\"Split train and valid dataset based on the No. of folder\"\"\"\r\n",
    "    folds = {0: [1, 3],\r\n",
    "             1: [2, 5],\r\n",
    "             2: [4, 8],\r\n",
    "             3: [6, 7]}\r\n",
    "    train_path = data_path / 'cropped_train'\r\n",
    "\r\n",
    "    train_file_names = []\r\n",
    "    val_file_names = []\r\n",
    "\r\n",
    "    for instrument_id in range(1, 9):\r\n",
    "        if instrument_id in folds[fold]:\r\n",
    "            val_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\r\n",
    "        else:\r\n",
    "            train_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\r\n",
    "\r\n",
    "    return train_file_names, val_file_names\r\n",
    "\r\n",
    "train_file_names, val_file_names = get_split(0)\r\n",
    "\r\n",
    "def load_image(path):\r\n",
    "    img = cv2.imread(str(path))\r\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
    "\r\n",
    "binary_factor = 255\r\n",
    "parts_factor = 85\r\n",
    "instrument_factor = 32\r\n",
    "\r\n",
    "def load_mask(path, problem_type=\"binary\", mask_folder=\"instruments_masks\",factor=instrument_factor):\r\n",
    "    if problem_type == 'binary':\r\n",
    "        mask_folder = 'binary_masks'\r\n",
    "        factor = binary_factor\r\n",
    "    elif problem_type == 'parts':\r\n",
    "        mask_folder = 'parts_masks'\r\n",
    "        factor = parts_factor\r\n",
    "    elif problem_type == 'instruments':\r\n",
    "        factor = instrument_factor\r\n",
    "        mask_folder = 'instruments_masks'\r\n",
    "\r\n",
    "    mask = cv2.imread(str(path).replace('images', mask_folder).replace('jpg', 'png'), 0)\r\n",
    "\r\n",
    "    return (mask / factor).astype(np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tau = 5\r\n",
    "train_img_path = [str(i) for i in train_file_names]\r\n",
    "train_frame_name = [i for i in train_img_path if int(i[-7:-4])>=tau]\r\n",
    "valid_img_path = [str(i) for i in val_file_names] \r\n",
    "valid_frame_name = [i for i in valid_img_path if int(i[-7:-4])>=tau]\r\n",
    "\r\n",
    "class InstrumentDataset(Dataset):\r\n",
    "    \"\"\"Dataset that loads multiple frame\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, file_names, problem_type=\"binary\", tau=5):\r\n",
    "        self.file_names = file_names\r\n",
    "        self.problem_type = problem_type\r\n",
    "        self.tau = tau      # tau is the number of frames should be combiend\r\n",
    "        self.transform = transforms.Compose([\r\n",
    "                                transforms.ToPILImage(),\r\n",
    "                                transforms.Resize([256,320]),\r\n",
    "                                transforms.ToTensor()\r\n",
    "                            ]) \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.file_names)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        current_frame = self.file_names[idx]\r\n",
    "        # mask = load_mask(current_frame, self.problem_type)\r\n",
    "        # mask = self.transform(mask)\r\n",
    "        frames_ls = []\r\n",
    "        masks_ls = []\r\n",
    "        for i in range(tau):\r\n",
    "            to_find = \"frame\"+current_frame[-7:-4]\r\n",
    "            to_repl = \"frame\"+ '%03d' % (int(current_frame[-7:-4])-i)\r\n",
    "            frame = current_frame.replace(to_find, to_repl)\r\n",
    "            frame_array = load_image(frame)\r\n",
    "            frame_tensor = self.transform(frame_array)\r\n",
    "            mask_array = load_mask(frame)\r\n",
    "            mask_tensor = self.transform(mask_array)\r\n",
    "            # Change the value in mask to 1 - 0 \r\n",
    "            mask_tensor = torch.where(mask_tensor>0,1,0)\r\n",
    "            # frame_tensor = torch.from_numpy(frame_tensor)            \r\n",
    "            frames_ls.append(frame_tensor)\r\n",
    "            masks_ls.append(mask_tensor)\r\n",
    "        frames_stack = torch.stack(frames_ls, 0)\r\n",
    "        masks_stack = torch.stack(masks_ls, 0)\r\n",
    "        # permute the tensor from [tau, H, W, C] to [tau, C, H, W]\r\n",
    "        # frames_tensor = frames_stack.permute(0,3,1,2) \r\n",
    "        return frames_stack.float(), masks_stack.float(), str(current_frame)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "training_data_frames = InstrumentDataset(train_frame_name,tau=tau)\r\n",
    "valid_data_frames = InstrumentDataset(valid_frame_name,tau=tau)\r\n",
    "\r\n",
    "batch_size = 1\r\n",
    "training_data_loader = Data.DataLoader(training_data_frames, batch_size=batch_size, shuffle=True)\r\n",
    "valid_data_loader = Data.DataLoader(valid_data_frames, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "a,b,c = next(iter(training_data_loader))\r\n",
    "print(f\"Data shape: {a.shape}\")\r\n",
    "print(f\"Mask shape: {b.shape}\")\r\n",
    "print(f\"Path: {c}\")\r\n",
    "# img = a[0][0].permute(1,2,0)\r\n",
    "# plt.imshow(img)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data shape: torch.Size([1, 5, 3, 256, 320])\n",
      "Mask shape: torch.Size([1, 5, 1, 256, 320])\n",
      "Path: ('C:\\\\Users\\\\Siyao\\\\Downloads\\\\EndoVis2017Data\\\\cropped_train\\\\instrument_dataset_4\\\\images\\\\frame015.jpg',)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class CNNBackbone(nn.Module):\r\n",
    "    def __init__(self, model=\"resnet101\", pretrained=True):\r\n",
    "        super(CNNBackbone, self).__init__()\r\n",
    "        self.model_name = model\r\n",
    "        self.pretrained = pretrained\r\n",
    "        if self.model_name == \"resnet101\" and pretrained:\r\n",
    "            model = models.resnet101(pretrained=True)\r\n",
    "            self.cnn = torch.nn.Sequential(*(list(model.children())[:-3])).eval()\r\n",
    "        else:\r\n",
    "            raise NotImplementedError(\"Please use some pretrained CNN models\")\r\n",
    "        \r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        y = self.cnn(x[0])\r\n",
    "        out = y.unsqueeze(0)\r\n",
    "        # out.shape = [batch_size, T, C, H, W] = [1, T, 1024, 16, 20]\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class PositionalEncoding3D(nn.Module):\r\n",
    "    \"\"\"https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/positional_encodings.py\"\"\"\r\n",
    "    \r\n",
    "    def __init__(self, channels=1024):\r\n",
    "        super(PositionalEncoding3D, self).__init__()\r\n",
    "        channels = int(np.ceil(channels/6)*2)\r\n",
    "        if channels % 2:\r\n",
    "            channels += 1\r\n",
    "        self.channels = channels\r\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\r\n",
    "        self.register_buffer('inv_freq', inv_freq)\r\n",
    "\r\n",
    "    def forward(self, tensor):\r\n",
    "        # Input tensor shape: [batch_size, T, C, H, W] \r\n",
    "        \"\"\"\r\n",
    "        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)\r\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)\r\n",
    "        \"\"\"\r\n",
    "        tensor = tensor.permute(0, 4, 3, 1, 2)\r\n",
    "        if len(tensor.shape) != 5:\r\n",
    "            raise RuntimeError(\"The input tensor has to be 5d!\")\r\n",
    "\r\n",
    "        batch_size, x, y, z, orig_ch = tensor.shape\r\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\r\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\r\n",
    "        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())\r\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\r\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\r\n",
    "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\r\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)\r\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)\r\n",
    "        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)\r\n",
    "        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())\r\n",
    "        emb[:,:,:,:self.channels] = emb_x\r\n",
    "        emb[:,:,:,self.channels:2*self.channels] = emb_y\r\n",
    "        emb[:,:,:,2*self.channels:] = emb_z\r\n",
    "        out = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)\r\n",
    "        out = out.permute(0, 3, 4, 2, 1)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class SparseAttention(nn.Module):\r\n",
    "    \"\"\"Sparse Self Attention Module\"\"\"\r\n",
    "    def __init__(self, in_dim=1024):\r\n",
    "        \"\"\"The only iuput attribute is dimension\"\"\"\r\n",
    "        super(SparseAttention, self).__init__()\r\n",
    "        self.query_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\r\n",
    "        self.key_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\r\n",
    "        self.value_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\r\n",
    "        self.softmax = nn.Softmax(dim=4)\r\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\r\n",
    "        self.affinity = torch.zeros(0)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # Input tensor shape: [batch_size, T, channel, H, W] \r\n",
    "        # Permute x to : [batch_size, channel, H, W, T]\r\n",
    "        x = x.permute(0, 2, 3, 4, 1)\r\n",
    "        b, C, H, W, T = x.shape\r\n",
    "        proj_query = self.query_conv(x)\r\n",
    "        proj_query_H = proj_query.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H).permute(0,2,1) # [b*W*T,H,C]\r\n",
    "        proj_query_W = proj_query.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W).permute(0,2,1) # [b*H*T,W,C]\r\n",
    "        proj_query_T = proj_query.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T).permute(0,2,1) # [b*W*H,T,C]\r\n",
    "\r\n",
    "        proj_key = self.key_conv(x)\r\n",
    "        proj_key_H = proj_key.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\r\n",
    "        proj_key_W = proj_key.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\r\n",
    "        proj_key_T = proj_key.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\r\n",
    "\r\n",
    "        proj_value = self.value_conv(x)\r\n",
    "        proj_value_H = proj_value.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\r\n",
    "        proj_value_W = proj_value.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\r\n",
    "        proj_value_T = proj_value.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\r\n",
    "\r\n",
    "        energy_H = torch.bmm(proj_query_H, proj_key_H).view(b,W,T,H,H).permute(0,3,1,2,4) # [b,H,W,T,H]\r\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(b,H,T,W,W).permute(0,1,3,2,4) # [b,H,W,T,W]\r\n",
    "        energy_T = torch.bmm(proj_query_T, proj_key_T).view(b,H,W,T,T)                    # [b,H,W,T,T]\r\n",
    "        score = self.softmax(torch.cat([energy_H,energy_W,energy_T],4))         # [b,H,W,T,(H+W+T)]\r\n",
    "        affinity = torch.max(score, 4).values\r\n",
    "        self.affinity = affinity.permute(0,3,1,2)\r\n",
    "        \r\n",
    "        att_H = score[:,:,:,:,0:H].permute(0,2,3,1,4).contiguous().view(b*W*T,H,H)      # [b*W*T,H,H]\r\n",
    "        att_W = score[:,:,:,:,H:H+W].permute(0,1,4,2,3).contiguous().view(b*H*T,W,W)    # [b*H*T,W,W]\r\n",
    "        att_T = score[:,:,:,:,H+W:].contiguous().view(b*H*W,T,T)                        # [b*H*W,T,T]\r\n",
    "\r\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0,2,1)).view(b,W,T,-1,H).permute(0,3,4,1,2)\r\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0,2,1)).view(b,H,T,-1,W).permute(0,3,1,4,2)\r\n",
    "        out_T = torch.bmm(proj_value_T, att_T.permute(0,2,1)).view(b,H,W,-1,T).permute(0,3,1,2,4)\r\n",
    "        \r\n",
    "        # permute back to [batch_size, T, channel, H, W] \r\n",
    "        output = self.gamma*(out_H + out_T + out_W).permute(0,4,1,2,3)\r\n",
    "\r\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class SSTEncoder(nn.Module):\r\n",
    "    \"\"\"Define the Multi-head attention -> Add&Norm -> Feed Forward -> Add&Norm module\"\"\"\r\n",
    "    def __init__(self, dim=1024, dropout=0.2):\r\n",
    "        super(SSTEncoder, self).__init__()\r\n",
    "\r\n",
    "        # Multi-head attention sub-layer\r\n",
    "        self.attn = SparseAttention(dim)\r\n",
    "        self.norm_1 = nn.LayerNorm(dim)\r\n",
    "        \r\n",
    "        # Feed forward sub-layer\r\n",
    "        self.fc = nn.Sequential(\r\n",
    "            nn.Linear(in_features=dim, out_features=dim*2),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(dropout),\r\n",
    "            nn.Linear(in_features=dim*2, out_features=dim)\r\n",
    "        )\r\n",
    "        self.norm_2 =  nn.LayerNorm(dim)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        y1 = self.attn(x)\r\n",
    "        x2 = (x+y1).permute(0,3,4,1,2)   # permute from [b,T,C,H,W] to [b,H,W,T,C]\r\n",
    "        y2 = self.norm_1(x2)\r\n",
    "        y3 = self.fc(y2)\r\n",
    "        out = self.norm_2(y2+y3).permute(0,3,4,1,2)  # permute from [b,H,W,T,C] to [b,T,C,H,W]\r\n",
    "\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class DecoderBlock(nn.Module):\r\n",
    "    \"\"\"Define Decoder block for deconvolution\"\"\"\r\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, deconv=True):\r\n",
    "        super(DecoderBlock, self).__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        if deconv:\r\n",
    "            self.Deblock = nn.Sequential(\r\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\r\n",
    "                nn.ReLU(),\r\n",
    "                nn.ConvTranspose2d(mid_channels, out_channels, kernel_size=4, stride=2, padding=1)\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            self.Deblock = nn.Sequential(\r\n",
    "                nn.Upsample(scale_factor=2, mode='bilinar'),\r\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\r\n",
    "                nn.ReLU(),\r\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\r\n",
    "                nn.ReLU(),\r\n",
    "            )\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = x.squeeze(0)\r\n",
    "        y = self.Deblock(x)\r\n",
    "        y = y.unsqueeze(0)\r\n",
    "        return y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "Decoder = DecoderBlock()\r\n",
    "Decoder.to(device)\r\n",
    "ad1 = Decoder(a.to(device))\r\n",
    "a.shape, ad1.shape"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'in_channels', 'mid_channels', and 'out_channels'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-968ce5c6faf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoderBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mad1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mad1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'in_channels', 'mid_channels', and 'out_channels'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class MyTransformer(nn.Module):\r\n",
    "    def __init__(self, num_layers=4):\r\n",
    "        super(MyTransformer, self).__init__()\r\n",
    "\r\n",
    "        self.backbone = CNNBackbone()\r\n",
    "        self.pos_encoding = PositionalEncoding3D()\r\n",
    "        self.self_attn1 = SSTEncoder()\r\n",
    "        self.self_attn2 = SSTEncoder()\r\n",
    "        self.self_attn3 = SSTEncoder()\r\n",
    "        self.self_attn4 = SSTEncoder()\r\n",
    "        \r\n",
    "        self.object_affinity = self.self_attn1.attn.affinity\r\n",
    "\r\n",
    "        self.dec1 = DecoderBlock(1024,1024,512)\r\n",
    "        self.dec2 = DecoderBlock(512,512,256)\r\n",
    "        self.dec3 = DecoderBlock(256,512,128)\r\n",
    "        self.dec4 = DecoderBlock(128,256,1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # Step 1: subtract feature embedding from CNN backbone\r\n",
    "        y1 = self.backbone(x)\r\n",
    "        # Step 2: Get positional encoding and add it to the feature embedding\r\n",
    "        pe = self.pos_encoding(y1)\r\n",
    "        y2 = pe + y1\r\n",
    "        # step 3: forward embedding in self-attention block\r\n",
    "        y3 = self.self_attn1(y2)\r\n",
    "        y4 = self.self_attn2(y3)\r\n",
    "        y5 = self.self_attn3(y4)\r\n",
    "        y6 = self.self_attn4(y5)\r\n",
    "        y7 = self.dec1(y6)\r\n",
    "        y8 = self.dec2(y7)\r\n",
    "        y9 = self.dec3(y8)\r\n",
    "        out = self.dec4(y9)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model = MyTransformer()\r\n",
    "model = model.to(device)\r\n",
    "z = model(a.to(device))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model.object_affinity"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "aff1 = model.self_attn1.attn.affinity\r\n",
    "aff2 = model.self_attn2.attn.affinity\r\n",
    "aff1.shape, aff2.shape\r\n",
    "a = torch.cat([aff1,aff2,aff1],0)\r\n",
    "a.shape"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ded793447458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maff1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maff2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maff1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maff2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maff1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maff2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maff1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "z0 = z.cpu().detach()\r\n",
    "img = z0[0][0].permute(1,2,0)\r\n",
    "plt.imshow(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "z0.shape, b.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Function and Criterion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LossBinary:\r\n",
    "    \"\"\"\r\n",
    "    Loss defined as \\alpha BCE - (1 - \\alpha) SoftJaccard\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, jaccard_weight=0):\r\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\r\n",
    "        self.jaccard_weight = jaccard_weight\r\n",
    "\r\n",
    "    def __call__(self, outputs, targets):\r\n",
    "        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\r\n",
    "\r\n",
    "        if self.jaccard_weight:\r\n",
    "            eps = 1e-15\r\n",
    "            jaccard_target = (targets == 1).float()\r\n",
    "            jaccard_output = F.sigmoid(outputs)\r\n",
    "\r\n",
    "            intersection = (jaccard_output * jaccard_target).sum()\r\n",
    "            union = jaccard_output.sum() + jaccard_target.sum()\r\n",
    "\r\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\r\n",
    "        return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, epochs):\r\n",
    "    # Initialize lists to store loss and fscore each epoch\r\n",
    "    LOSS_train = []\r\n",
    "    LOSS_valid = []\r\n",
    "    IOU_train = []\r\n",
    "    IOU_valid = []  \r\n",
    "\r\n",
    "    for epoch in tqdm.trange(epochs, desc=\"Epochs\"):\r\n",
    "        result = []\r\n",
    "        train_loss = 0.0\r\n",
    "        train_IOU = 0.0\r\n",
    "        valid_loss = 0.0\r\n",
    "        valid_IOU = 0.0\r\n",
    "        # Begin training\r\n",
    "        model.train()\r\n",
    "        for data, mask, _ in train_dataloader:\r\n",
    "            data, mask = data.to(device), mask.to(device)\r\n",
    "\r\n",
    "            output = model(data)                            # Forward Passing\r\n",
    "            # output = output[0][0].unsqueeze(0)\r\n",
    "            loss = criterion(output, mask)                  # Compute loss\r\n",
    "            # preds = torch.sigmoid(output) > 0.5             # Make prediction\r\n",
    "            # preds = preds.to(torch.float32)                 # Convert to float32 torch\r\n",
    "            loss.backward()                                 # Compute gradients\r\n",
    "            optimizer.step()                                # Update the model parameters\r\n",
    "            scheduler.step()                                # Update learning rate with scheduler\r\n",
    "            optimizer.zero_grad()                           # Clear the gradients\r\n",
    "            train_loss += loss.item() * data.size(0)        # Compute training loss\r\n",
    "\r\n",
    "            \r\n",
    "            # train_IOU += get_jaccard(mask.to(\"cpu\").to(torch.float16).numpy(), \r\n",
    "            #                      preds.to(\"cpu\").to(torch.float16).numpy()) * data.size(0)\r\n",
    "\r\n",
    "        # Begin validation\r\n",
    "        model.eval()\r\n",
    "        for data, mask, _ in valid_dataloader:\r\n",
    "            data, mask = data.to(device), mask.to(device)\r\n",
    "\r\n",
    "            output = model(data)                            # Forward Passing\r\n",
    "            # output = output[0][0].unsqueeze(0)\r\n",
    "        \r\n",
    "            loss = criterion(output, mask)                  # Compute loss\r\n",
    "            # preds = torch.sigmoid(output) > 0.5             # Make prediction\r\n",
    "            # preds = preds.to(torch.float32)                 # Convert to float32 torch\r\n",
    "            valid_loss += loss.item() * data.size(0)        # Compute validation loss\r\n",
    "            # valid_IOU += get_jaccard(mask.to(\"cpu\").to(torch.float16).numpy(), \r\n",
    "            #                      preds.to(\"cpu\").to(torch.float16).numpy()) * data.size(0)\r\n",
    "        \r\n",
    "        # Compute epoch loss and f1\r\n",
    "        epoch_train_loss = train_loss / len(train_dataloader.dataset)\r\n",
    "        # epoch_train_IoU = train_IOU / len(train_dataloader.dataset)\r\n",
    "        epoch_valid_loss = valid_loss / len(valid_dataloader.dataset)\r\n",
    "        # epoch_valid_IoU = valid_IOU / len(valid_dataloader.dataset)\r\n",
    "\r\n",
    "        # Record epoch loss and f1 to the list\r\n",
    "        LOSS_train.append(epoch_train_loss)\r\n",
    "        LOSS_valid.append(epoch_valid_loss)\r\n",
    "        # IOU_train.append(epoch_train_IoU)\r\n",
    "        # IOU_valid.append(epoch_valid_IoU)       \r\n",
    "\r\n",
    "        # result.append(f'{epoch} TRAIN loss: {epoch_train_loss:.4f}, IOU: {epoch_train_IoU:.4f}   VALID loss: {epoch_valid_loss:.4f}, IOU: {epoch_valid_IoU:.4f}')\r\n",
    "        result.append(f'{epoch} TRAIN loss: {epoch_train_loss:.4f}, VALID loss: {epoch_valid_loss:.4f}, ')\r\n",
    "\r\n",
    "\r\n",
    "        print(result)\r\n",
    "    return LOSS_train, LOSS_valid, IOU_train, IOU_valid"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = LossBinary(0.5)\r\n",
    "# criterion = criterion.to(device)\r\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\r\n",
    "sgdr_partial = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.01)\r\n",
    "LOSS_train, LOSS_valid, FSCORE_train, FSCORE_valid = train(model, training_data_loader, valid_data_loader, criterion, optimizer, sgdr_partial, epochs=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frame,mask,c = next(iter(training_data_loader))\r\n",
    "print(f\"Data shape: {frame.shape}\")\r\n",
    "print(f\"Mask shape: {mask.shape}\")\r\n",
    "print(f\"Path: {c}\")\r\n",
    "# img = a[0][0].permute(1,2,0)\r\n",
    "# plt.imshow(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mask.unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_mask(img):\r\n",
    "    # with torch.no_grad():\r\n",
    "    model.eval()\r\n",
    "    img = img.to(device)\r\n",
    "    prd = model(img)\r\n",
    "    return prd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = get_mask(frame)\r\n",
    "pred = torch.sigmoid(pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "msk0 = mask.detach().cpu()\r\n",
    "msk1 = msk0[0][0].permute(1,2,0)\r\n",
    "plt.imshow(msk1>0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd0 = pred.detach().cpu()\r\n",
    "prd1 = prd0[0][0].permute(1,2,0)\r\n",
    "plt.imshow(prd1>0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inp = torch.randn(3, 5, requires_grad=True)\r\n",
    "tar = torch.empty(3, dtype=torch.long).random_(5)\r\n",
    "inp.shape, tar.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred.shape, mask.shape\r\n",
    "pred = pred.unsqueeze(0)\r\n",
    "mask = mask.unsqueeze(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LossBinary:\r\n",
    "    \"\"\"\r\n",
    "    Loss defined as \\alpha BCE - (1 - \\alpha) SoftJaccard\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, jaccard_weight=0):\r\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\r\n",
    "        self.jaccard_weight = jaccard_weight\r\n",
    "\r\n",
    "    def __call__(self, outputs, targets):\r\n",
    "        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\r\n",
    "\r\n",
    "        if self.jaccard_weight:\r\n",
    "            eps = 1e-15\r\n",
    "            jaccard_target = (targets == 1).float()\r\n",
    "            jaccard_output = F.sigmoid(outputs)\r\n",
    "\r\n",
    "            intersection = (jaccard_output * jaccard_target).sum()\r\n",
    "            union = jaccard_output.sum() + jaccard_target.sum()\r\n",
    "\r\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\r\n",
    "        return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = LossBinary(0.5)\r\n",
    "loss = criterion(prd1.cuda(), msk1.cuda())\r\n",
    "loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "msk1.count_nonzero()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "msk1.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "256*320"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(mask[0].permute(1,2,0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd = pred[0][0].detach().to('cpu')\r\n",
    "prd = prd.permute(1,2,0)\r\n",
    "plt.imshow(prd>0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('datasci': conda)"
  },
  "interpreter": {
   "hash": "23d8489b17aa989cee42bd3f3e82cc035b4cfa42fa1ea343caf5051171f57614"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}