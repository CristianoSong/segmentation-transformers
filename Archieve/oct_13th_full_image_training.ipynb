{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-11\n",
    "- Try to train the model in full image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import tqdm\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"C:/Users/Siyao/Downloads/EndoVis2017Data\")\n",
    "train_path = data_path / \"cropped_train\"\n",
    "\n",
    "def get_split(fold):\n",
    "    \"\"\"Split train and valid dataset based on the No. of folder\"\"\"\n",
    "    folds = {0: [1, 3],\n",
    "             1: [2, 5],\n",
    "             2: [4, 8],\n",
    "             3: [6, 7]}\n",
    "    train_path = data_path / 'cropped_train'\n",
    "\n",
    "    train_file_names = []\n",
    "    val_file_names = []\n",
    "\n",
    "    for instrument_id in range(1, 9):\n",
    "        if instrument_id in folds[fold]:\n",
    "            val_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n",
    "        else:\n",
    "            train_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n",
    "\n",
    "    return train_file_names, val_file_names\n",
    "\n",
    "train_file_names, val_file_names = get_split(0)\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "binary_factor = 255\n",
    "parts_factor = 85\n",
    "instrument_factor = 32\n",
    "\n",
    "def load_mask(path, problem_type=\"binary\", mask_folder=\"instruments_masks\",factor=instrument_factor):\n",
    "    if problem_type == 'binary':\n",
    "        mask_folder = 'binary_masks'\n",
    "        factor = binary_factor\n",
    "    elif problem_type == 'parts':\n",
    "        mask_folder = 'parts_masks'\n",
    "        factor = parts_factor\n",
    "    elif problem_type == 'instruments':\n",
    "        factor = instrument_factor\n",
    "        mask_folder = 'instruments_masks'\n",
    "\n",
    "    mask = cv2.imread(str(path).replace('images', mask_folder).replace('jpg', 'png'), 0)\n",
    "\n",
    "    return (mask / factor).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "train_img_path = [str(i) for i in train_file_names]\n",
    "train_frame_name = [i for i in train_img_path if int(i[-7:-4])>=tau]\n",
    "valid_img_path = [str(i) for i in val_file_names] \n",
    "valid_frame_name = [i for i in valid_img_path if int(i[-7:-4])>=tau]\n",
    "\n",
    "class InstrumentDataset(Dataset):\n",
    "    \"\"\"Dataset that loads multiple frame\"\"\"\n",
    "\n",
    "    def __init__(self, file_names, problem_type=\"binary\", tau=5):\n",
    "        self.file_names = file_names\n",
    "        self.problem_type = problem_type\n",
    "        self.tau = tau      # tau is the number of frames should be combiend\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.ToPILImage(),\n",
    "                                # transforms.Resize([256,320]),\n",
    "                                transforms.ToTensor()\n",
    "                            ]) \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_frame = self.file_names[idx]\n",
    "        # mask = load_mask(current_frame, self.problem_type)\n",
    "        # mask = self.transform(mask)\n",
    "        frames_ls = []\n",
    "        masks_ls = []\n",
    "        for i in range(tau):\n",
    "            to_find = \"frame\"+current_frame[-7:-4]\n",
    "            to_repl = \"frame\"+ '%03d' % (int(current_frame[-7:-4])-i)\n",
    "            frame = current_frame.replace(to_find, to_repl)\n",
    "            frame_array = load_image(frame)\n",
    "            frame_tensor = self.transform(frame_array)\n",
    "            mask_array = load_mask(frame)\n",
    "            mask_tensor = self.transform(mask_array)\n",
    "            # Change the value in mask to 1 - 0 \n",
    "            mask_tensor = torch.where(mask_tensor>0,1,0)\n",
    "            # frame_tensor = torch.from_numpy(frame_tensor)            \n",
    "            frames_ls.append(frame_tensor)\n",
    "            masks_ls.append(mask_tensor)\n",
    "        frames_stack = torch.stack(frames_ls, 0)\n",
    "        masks_stack = torch.stack(masks_ls, 0)\n",
    "        # permute the tensor from [tau, H, W, C] to [tau, C, H, W]\n",
    "        # frames_tensor = frames_stack.permute(0,3,1,2) \n",
    "        return frames_stack.float(), masks_stack.float(), str(current_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frames = InstrumentDataset(train_frame_name,tau=tau)\n",
    "valid_data_frames = InstrumentDataset(valid_frame_name,tau=tau)\n",
    "\n",
    "batch_size = 1\n",
    "training_data_loader = Data.DataLoader(training_data_frames, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = Data.DataLoader(valid_data_frames, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = next(iter(training_data_loader))\n",
    "print(f\"Data shape: {a.shape}\")\n",
    "print(f\"Mask shape: {b.shape}\")\n",
    "print(f\"Path: {c}\")\n",
    "# img = a[0][0].permute(1,2,0)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self, model=\"resnet101\", pretrained=True):\n",
    "        super(CNNBackbone, self).__init__()\n",
    "        self.model_name = model\n",
    "        self.pretrained = pretrained\n",
    "        if self.model_name == \"resnet101\" and pretrained:\n",
    "            model = models.resnet101(pretrained=True)\n",
    "            self.cnn = torch.nn.Sequential(*(list(model.children())[:-4])).eval()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please use some pretrained CNN models\")\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.cnn(x[0])\n",
    "        out = y.unsqueeze(0)\n",
    "        # out.shape = [batch_size, T, C, H, W] = [1, T, 1024, 16, 20]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/positional_encodings.py\"\"\"\n",
    "    \n",
    "    def __init__(self, channels=1024):\n",
    "        super(PositionalEncoding3D, self).__init__()\n",
    "        channels = int(np.ceil(channels/6)*2)\n",
    "        if channels % 2:\n",
    "            channels += 1\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # Input tensor shape: [batch_size, T, C, H, W] \n",
    "        \"\"\"\n",
    "        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 4, 3, 1, 2)\n",
    "        if len(tensor.shape) != 5:\n",
    "            raise RuntimeError(\"The input tensor has to be 5d!\")\n",
    "\n",
    "        batch_size, x, y, z, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)\n",
    "        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:,:,:self.channels] = emb_x\n",
    "        emb[:,:,:,self.channels:2*self.channels] = emb_y\n",
    "        emb[:,:,:,2*self.channels:] = emb_z\n",
    "        out = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)\n",
    "        out = out.permute(0, 3, 4, 2, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttention(nn.Module):\n",
    "    \"\"\"Sparse Self Attention Module\"\"\"\n",
    "    def __init__(self, in_dim=1024):\n",
    "        \"\"\"The only iuput attribute is dimension\"\"\"\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=4)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.affinity = torch.zeros(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input tensor shape: [batch_size, T, channel, H, W] \n",
    "        # Permute x to : [batch_size, channel, H, W, T]\n",
    "        x = x.permute(0, 2, 3, 4, 1)\n",
    "        b, C, H, W, T = x.shape\n",
    "        proj_query = self.query_conv(x)\n",
    "        proj_query_H = proj_query.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H).permute(0,2,1) # [b*W*T,H,C]\n",
    "        proj_query_W = proj_query.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W).permute(0,2,1) # [b*H*T,W,C]\n",
    "        proj_query_T = proj_query.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T).permute(0,2,1) # [b*W*H,T,C]\n",
    "\n",
    "        proj_key = self.key_conv(x)\n",
    "        proj_key_H = proj_key.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\n",
    "        proj_key_W = proj_key.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\n",
    "        proj_key_T = proj_key.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\n",
    "\n",
    "        proj_value = self.value_conv(x)\n",
    "        proj_value_H = proj_value.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\n",
    "        proj_value_W = proj_value.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\n",
    "        proj_value_T = proj_value.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\n",
    "\n",
    "        energy_H = torch.bmm(proj_query_H, proj_key_H).view(b,W,T,H,H).permute(0,3,1,2,4) # [b,H,W,T,H]\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(b,H,T,W,W).permute(0,1,3,2,4) # [b,H,W,T,W]\n",
    "        energy_T = torch.bmm(proj_query_T, proj_key_T).view(b,H,W,T,T)                    # [b,H,W,T,T]\n",
    "        score = self.softmax(torch.cat([energy_H,energy_W,energy_T],4))         # [b,H,W,T,(H+W+T)]\n",
    "        affinity = torch.max(score, 4).values\n",
    "        self.affinity = affinity.permute(0,3,1,2)\n",
    "        \n",
    "        att_H = score[:,:,:,:,0:H].permute(0,2,3,1,4).contiguous().view(b*W*T,H,H)      # [b*W*T,H,H]\n",
    "        att_W = score[:,:,:,:,H:H+W].permute(0,1,4,2,3).contiguous().view(b*H*T,W,W)    # [b*H*T,W,W]\n",
    "        att_T = score[:,:,:,:,H+W:].contiguous().view(b*H*W,T,T)                        # [b*H*W,T,T]\n",
    "\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0,2,1)).view(b,W,T,-1,H).permute(0,3,4,1,2)\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0,2,1)).view(b,H,T,-1,W).permute(0,3,1,4,2)\n",
    "        out_T = torch.bmm(proj_value_T, att_T.permute(0,2,1)).view(b,H,W,-1,T).permute(0,3,1,2,4)\n",
    "        \n",
    "        # permute back to [batch_size, T, channel, H, W] \n",
    "        output = self.gamma*(out_H + out_T + out_W).permute(0,4,1,2,3)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTEncoder(nn.Module):\n",
    "    \"\"\"Define the Multi-head attention -> Add&Norm -> Feed Forward -> Add&Norm module\"\"\"\n",
    "    def __init__(self, dim=512, dropout=0.2):\n",
    "        super(SSTEncoder, self).__init__()\n",
    "\n",
    "        # Multi-head attention sub-layer\n",
    "        self.attn = SparseAttention(dim)\n",
    "        self.norm_1 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Feed forward sub-layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=dim*2, out_features=dim)\n",
    "        )\n",
    "        self.norm_2 =  nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y1 = self.attn(x)\n",
    "        x2 = (x+y1).permute(0,3,4,1,2)   # permute from [b,T,C,H,W] to [b,H,W,T,C]\n",
    "        y2 = self.norm_1(x2)\n",
    "        y3 = self.fc(y2)\n",
    "        out = self.norm_2(y2+y3).permute(0,3,4,1,2)  # permute from [b,H,W,T,C] to [b,T,C,H,W]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Define Decoder block for deconvolution\"\"\"\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, deconv=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # self.in_channels = in_channels\n",
    "        if deconv:\n",
    "            self.Deblock = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(mid_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(mid_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            self.Deblock = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinar'),\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "        y = self.Deblock(x)\n",
    "        y = y.unsqueeze(0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, num_layers=4):\n",
    "        super(MyTransformer, self).__init__()\n",
    "\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.pos_encoding = PositionalEncoding3D()\n",
    "        self.self_attn1 = SSTEncoder()\n",
    "        self.self_attn2 = SSTEncoder()\n",
    "        self.self_attn3 = SSTEncoder()\n",
    "        self.self_attn4 = SSTEncoder()\n",
    "        self.dec1 = DecoderBlock(512*2+4,512,256)\n",
    "        self.dec2 = DecoderBlock(256,128,64)\n",
    "        self.dec3 = DecoderBlock(64,32,1)\n",
    "        # self.dec4 = DecoderBlock(128,256,1)\n",
    "        # self.object_affinity = torch.cat([self.self_attn1.attn.affinity,\n",
    "        #                                   self.self_attn2.attn.affinity,\n",
    "        #                                   self.self_attn3.attn.affinity,\n",
    "        #                                   self.self_attn4.attn.affinity], 0)]\n",
    "        self.cnn_feat = torch.zeros(1)\n",
    "        self.encod_feat = torch.zeros(1)\n",
    "        self.obj_aff = torch.zeros(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. subtract feature embedding from CNN backbone [b,T,C,H',W']\n",
    "        cnn_fs = self.backbone(x)\n",
    "        self.cnn_feat = cnn_fs\n",
    "        # Get positional encoding and add it to the feature embedding\n",
    "        pe = self.pos_encoding(cnn_fs)\n",
    "        y = pe + cnn_fs\n",
    "\n",
    "        # 2. Encoded feature from transformer layers \n",
    "        y = self.self_attn1(y)\n",
    "        y = self.self_attn2(y)\n",
    "        y = self.self_attn3(y)\n",
    "        T_L = self.self_attn4(y)\n",
    "        self.encod_feat = T_L\n",
    "        \n",
    "        # 3. Object Affinity Value [N_layers,T,H',W']:\n",
    "        obj_aff = torch.cat([self.self_attn1.attn.affinity,\n",
    "                                          self.self_attn2.attn.affinity,\n",
    "                                          self.self_attn3.attn.affinity,\n",
    "                                          self.self_attn4.attn.affinity], 0)\n",
    "\n",
    "        # Adjust to [b,T,C,H',W']\n",
    "        obj_aff = obj_aff.transpose(0,1).unsqueeze(0)\n",
    "        self.obj_aff = obj_aff\n",
    "\n",
    "        # Concatnate feature together\n",
    "        emb = torch.cat([cnn_fs, T_L, obj_aff],dim=2)\n",
    "        # print(y.shape,T_L.shape,obj_aff.shape)\n",
    "        \n",
    "        z = self.dec1(emb)\n",
    "        z = self.dec2(z)\n",
    "        z = self.dec3(z)\n",
    "        # z = self.dec4(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyTransformer()\n",
    "model = model.to(device)\n",
    "# z = model(a.to(device))\n",
    "# z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder = DecoderBlock(2052,1024,1)\n",
    "# Decoder.to(device)\n",
    "# zd1 = Decoder(z)\n",
    "# z.shape, zd1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = a[0][-1].permute(1,2,0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = b[0][-1].permute(1,2,0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = model.cnn_feat[0][0].detach().cpu().permute(1,2,0)\n",
    "img = torch.sum(img, 2)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (model.encod_feat+model.cnn_feat)[0][0].detach().cpu().permute(1,2,0)\n",
    "img = torch.sum(img, 2)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = z.cpu().detach()\n",
    "img = z0[0][0].permute(1,2,0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    intersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1)\n",
    "    union = y_true.sum(dim=-2).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1)\n",
    "    jaccard = ((intersection + epsilon) / (union - intersection + epsilon)).data.cpu().numpy()\n",
    "    return jaccard[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossBinary:\n",
    "    \"\"\"\n",
    "    Loss defined as \\alpha BCE - (1 - \\alpha) SoftJaccard\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jaccard_weight=0):\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
    "        self.jaccard_weight = jaccard_weight\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\n",
    "\n",
    "        if self.jaccard_weight:\n",
    "            eps = 1e-15\n",
    "            jaccard_target = (targets == 1).float()\n",
    "            jaccard_output = torch.sigmoid(outputs)\n",
    "\n",
    "            intersection = (jaccard_output * jaccard_target).sum()\n",
    "            union = jaccard_output.sum() + jaccard_target.sum()\n",
    "\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, criterion, optimizer, epochs):\n",
    "    # Initialize lists to store loss and fscore each epoch\n",
    "    LOSS_train = []\n",
    "    LOSS_valid = []\n",
    "    Jaccard_train = []\n",
    "    Jaccard_valid = []  \n",
    "\n",
    "    for epoch in tqdm.trange(epochs, desc=\"Epochs\"):\n",
    "        result = []\n",
    "        train_loss = 0.0\n",
    "        train_Jaccard = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_Jaccard = 0.0\n",
    "        # Begin training\n",
    "        model.train()\n",
    "        for data, mask, _ in train_dataloader:\n",
    "            data, mask = data.to(device), mask.to(device)\n",
    "\n",
    "            output = model(data)                            # Forward Passing\n",
    "            loss = criterion(output, mask)                  # Compute loss\n",
    "            preds = torch.sigmoid(output) > 0.5             # Make prediction\n",
    "            preds = preds.to(torch.float32)                 # Convert to float32 torch\n",
    "            loss.backward()                                 # Compute gradients\n",
    "            optimizer.step()                                # Update the model parameters\n",
    "            # scheduler.step()                                # Update learning rate with scheduler\n",
    "            optimizer.zero_grad()                           # Clear the gradients\n",
    "            train_loss += loss.item() * data.size(0)        # Compute training loss\n",
    "            train_Jaccard += get_jaccard(mask[0][-1], preds[0][-1])\n",
    "\n",
    "        # Begin validation\n",
    "        model.eval()\n",
    "        for data, mask, _ in valid_dataloader:\n",
    "            data, mask = data.to(device), mask.to(device)\n",
    "\n",
    "            output = model(data)                            # Forward Passing      \n",
    "            loss = criterion(output, mask)                  # Compute loss\n",
    "            preds = torch.sigmoid(output) > 0.5             # Make prediction\n",
    "            preds = preds.to(torch.float32)                 # Convert to float32 torch\n",
    "            valid_loss += loss.item() * data.size(0)        # Compute validation loss\n",
    "            valid_Jaccard += get_jaccard(mask[0][-1], preds[0][-1])\n",
    "        \n",
    "        # Compute epoch loss and f1\n",
    "        epoch_train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        epoch_train_Jaccard = train_Jaccard / len(train_dataloader.dataset)\n",
    "        epoch_valid_loss = valid_loss / len(valid_dataloader.dataset)\n",
    "        epoch_valid_Jaccard = valid_Jaccard / len(valid_dataloader.dataset)\n",
    "\n",
    "        # Record epoch loss and f1 to the list\n",
    "        LOSS_train.append(epoch_train_loss)\n",
    "        LOSS_valid.append(epoch_valid_loss)\n",
    "        Jaccard_train.append(epoch_train_Jaccard)\n",
    "        Jaccard_valid.append(epoch_valid_Jaccard)       \n",
    "\n",
    "        result.append(f'{epoch} TRAIN loss: {epoch_train_loss:.4f}, Jaccard: {epoch_train_Jaccard:.4f}   VALID loss: {epoch_valid_loss:.4f}, Jaccard: {epoch_valid_Jaccard:.4f}')\n",
    "        # result.append(f'{epoch} TRAIN loss: {epoch_train_loss:.4f}, VALID loss: {epoch_valid_loss:.4f}, ')\n",
    "\n",
    "        print(result)\n",
    "    return LOSS_train, LOSS_valid, Jaccard_train, Jaccard_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [28:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 24.00 GiB total capacity; 21.23 GiB already allocated; 0 bytes free; 21.62 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-56a4d242f649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# criterion = nn.BCEWithLogitsLoss()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mLOSS_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOSS_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFSCORE_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFSCORE_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-84a9da44b52b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, valid_dataloader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m                            \u001b[1;31m# Forward Passing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m                  \u001b[1;31m# Compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m             \u001b[1;31m# Make prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\datasci\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-199f574daecb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# 2. Encoded feature from transformer layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\datasci\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c7abbf9547d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0my3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# permute from [b,H,W,T,C] to [b,T,C,H,W]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 200.00 MiB (GPU 0; 24.00 GiB total capacity; 21.23 GiB already allocated; 0 bytes free; 21.62 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "criterion = LossBinary(0.5)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "LOSS_train, LOSS_valid, FSCORE_train, FSCORE_valid = train(model, training_data_loader, valid_data_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame,mask,c = training_data_frames[300]\n",
    "print(f\"Data shape: {frame.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Path: {c}\")\n",
    "# img = a[0][0].permute(1,2,0)\n",
    "# plt.imshow(img)\n",
    "frame = frame.unsqueeze(0)\n",
    "mask = mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    # with torch.no_grad():\n",
    "    model.eval()\n",
    "    img = img.to(device)\n",
    "    prd = model(img)\n",
    "    return prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = get_mask(frame)\n",
    "pred = torch.sigmoid(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = frame[0][0].permute(1,2,0)\n",
    "plt.imshow(frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk0 = mask.detach().cpu()\n",
    "msk1 = msk0[0][0].permute(1,2,0)\n",
    "plt.imshow(msk1>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd0 = pred.detach().cpu()\n",
    "prd1 = prd0[0][0].permute(1,2,0)\n",
    "plt.imshow(prd1>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(3, 5, requires_grad=True)\n",
    "tar = torch.empty(3, dtype=torch.long).random_(5)\n",
    "inp.shape, tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape, mask.shape\n",
    "pred = pred.unsqueeze(0)\n",
    "mask = mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossBinary:\n",
    "    \"\"\"\n",
    "    Loss defined as \\alpha BCE - (1 - \\alpha) SoftJaccard\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jaccard_weight=0):\n",
    "        self.nll_loss = nn.BCEWithLogitsLoss()\n",
    "        self.jaccard_weight = jaccard_weight\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\n",
    "\n",
    "        if self.jaccard_weight:\n",
    "            eps = 1e-15\n",
    "            jaccard_target = (targets == 1).float()\n",
    "            jaccard_output = F.sigmoid(outputs)\n",
    "\n",
    "            intersection = (jaccard_output * jaccard_target).sum()\n",
    "            union = jaccard_output.sum() + jaccard_target.sum()\n",
    "\n",
    "            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LossBinary(0.5)\n",
    "loss = criterion(prd1.cuda(), msk1.cuda())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk1.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256*320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23d8489b17aa989cee42bd3f3e82cc035b4cfa42fa1ea343caf5051171f57614"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('datasci': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
