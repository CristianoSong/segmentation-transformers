{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oct 25th - Binary Task\n",
    "- Object affinity V2: use a similar computation method as score x value in attention layer\n",
    "- Should be the correct way to implement mask information in the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import tqdm\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"C:/Users/Siyao/Downloads/EndoVis2017Data\")\n",
    "train_path = data_path / \"cropped_train\"\n",
    "\n",
    "def get_split(fold):\n",
    "    \"\"\"Split train and valid dataset based on the No. of folder\"\"\"\n",
    "    folds = {0: [1, 3],\n",
    "             1: [2, 5],\n",
    "             2: [4, 8],\n",
    "             3: [6, 7]}\n",
    "    train_path = data_path / 'cropped_train'\n",
    "\n",
    "    train_file_names = []\n",
    "    val_file_names = []\n",
    "\n",
    "    for instrument_id in range(1, 9):\n",
    "        if instrument_id in folds[fold]:\n",
    "            val_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n",
    "        else:\n",
    "            train_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n",
    "\n",
    "    return train_file_names, val_file_names\n",
    "\n",
    "train_file_names, val_file_names = get_split(0)\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "binary_factor = 255\n",
    "parts_factor = 85\n",
    "instrument_factor = 32\n",
    "\n",
    "def load_mask(path, problem_type=\"binary\", mask_folder=\"instruments_masks\",factor=instrument_factor):\n",
    "    if problem_type == 'binary':\n",
    "        mask_folder = 'binary_masks'\n",
    "        factor = binary_factor\n",
    "    elif problem_type == 'parts':\n",
    "        mask_folder = 'parts_masks'\n",
    "        factor = parts_factor\n",
    "    elif problem_type == 'instruments':\n",
    "        factor = instrument_factor\n",
    "        mask_folder = 'instruments_masks'\n",
    "\n",
    "    mask = cv2.imread(str(path).replace('images', mask_folder).replace('jpg', 'png'), 0)\n",
    "\n",
    "    return (mask / factor).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5\n",
    "train_img_path = [str(i) for i in train_file_names]\n",
    "train_frame_name = [i for i in train_img_path if int(i[-7:-4])>=tau-1]\n",
    "valid_img_path = [str(i) for i in val_file_names] \n",
    "valid_frame_name = [i for i in valid_img_path if int(i[-7:-4])>=tau-1]\n",
    "\n",
    "class InstrumentDataset(Dataset):\n",
    "    \"\"\"Dataset that loads multiple frame\"\"\"\n",
    "\n",
    "    def __init__(self, file_names, problem_type=\"binary\", tau=5):\n",
    "        self.file_names = file_names\n",
    "        self.problem_type = problem_type\n",
    "        self.tau = tau      # tau is the number of frames should be combiend\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.ToPILImage(),\n",
    "                                transforms.Resize([256,320]),\n",
    "                                transforms.ToTensor()\n",
    "                            ]) \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_frame = self.file_names[idx]\n",
    "        # mask = load_mask(current_frame, self.problem_type)\n",
    "        # mask = self.transform(mask)\n",
    "        frames_ls = []\n",
    "        masks_ls = []\n",
    "        preds_ls = []\n",
    "        for i in range(tau):\n",
    "            to_find = \"frame\"+current_frame[-7:-4]\n",
    "            to_repl = \"frame\"+ '%03d' % (int(current_frame[-7:-4])-i)\n",
    "            frame = current_frame.replace(to_find, to_repl)\n",
    "            frame_array = load_image(frame)\n",
    "            frame_tensor = self.transform(frame_array)\n",
    "            mask_array = load_mask(frame,problem_type=self.problem_type)\n",
    "            mask_tensor = torch.from_numpy(mask_array)\n",
    "            # mask_tensor = self.transform(mask_array)\n",
    "            # Change the value in mask to 1 - 0 \n",
    "            # mask_tensor = torch.where(mask_tensor>0,1,0) \n",
    "            prediction = frame.replace(\"images\",\"prediction_temp\").replace(\".jpg\",\".pt\")\n",
    "            try:\n",
    "                torch.load(prediction)\n",
    "            except:\n",
    "                pred_tensor = torch.zeros(1,32,40)\n",
    "            else:\n",
    "                pred_tensor = torch.load(prediction)        \n",
    "            frames_ls.append(frame_tensor)\n",
    "            masks_ls.append(mask_tensor)\n",
    "            preds_ls.append(pred_tensor)\n",
    "        frames_stack = torch.stack(frames_ls, 0)\n",
    "        masks_stack = torch.stack(masks_ls, 0)\n",
    "        preds_stack = torch.stack(preds_ls, 0)\n",
    "        # permute the tensor from [tau, H, W, C] to [tau, C, H, W]\n",
    "        # frames_tensor = frames_stack.permute(0,3,1,2) \n",
    "        return frames_stack.float(), masks_stack.long(), preds_stack.float(), str(current_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frames = InstrumentDataset(train_frame_name,problem_type=\"binary\",tau=tau)\n",
    "valid_data_frames = InstrumentDataset(valid_frame_name,problem_type=\"binary\",tau=tau)\n",
    "\n",
    "batch_size = 1\n",
    "training_data_loader = Data.DataLoader(training_data_frames, batch_size=batch_size, shuffle=False)\n",
    "valid_data_loader = Data.DataLoader(valid_data_frames, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1, 5, 3, 256, 320])\n",
      "Mask shape: torch.Size([1, 5, 1024, 1280])\n",
      "Pred shape: torch.Size([1, 5, 1, 32, 40])\n",
      "Path: ('C:\\\\Users\\\\Siyao\\\\Downloads\\\\EndoVis2017Data\\\\cropped_train\\\\instrument_dataset_2\\\\images\\\\frame004.jpg',)\n"
     ]
    }
   ],
   "source": [
    "a,b,b0,c = next(iter(training_data_loader))\n",
    "print(f\"Data shape: {a.shape}\")\n",
    "print(f\"Mask shape: {b.shape}\")\n",
    "print(f\"Pred shape: {b0.shape}\")\n",
    "print(f\"Path: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self, model=\"resnet101\", pretrained=True):\n",
    "        super(CNNBackbone, self).__init__()\n",
    "        self.model_name = model\n",
    "        self.pretrained = pretrained\n",
    "        if self.model_name == \"resnet101\" and pretrained:\n",
    "            model = models.resnet101(pretrained=True)\n",
    "            self.cnn = torch.nn.Sequential(*(list(model.children())[:-4])).eval()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please use some pretrained CNN models\")\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.cnn(x[0])\n",
    "        out = y.unsqueeze(0)\n",
    "        # out.shape = [batch_size, T, C, H, W] = [1, T, 1024, 16, 20]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/positional_encodings.py\"\"\"\n",
    "    \n",
    "    def __init__(self, channels=1024):\n",
    "        super(PositionalEncoding3D, self).__init__()\n",
    "        channels = int(np.ceil(channels/6)*2)\n",
    "        if channels % 2:\n",
    "            channels += 1\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # Input tensor shape: [batch_size, T, C, H, W] \n",
    "        \"\"\"\n",
    "        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 4, 3, 1, 2)\n",
    "        if len(tensor.shape) != 5:\n",
    "            raise RuntimeError(\"The input tensor has to be 5d!\")\n",
    "\n",
    "        batch_size, x, y, z, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1).unsqueeze(1).unsqueeze(1)\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1).unsqueeze(1)\n",
    "        emb_z = torch.cat((sin_inp_z.sin(), sin_inp_z.cos()), dim=-1)\n",
    "        emb = torch.zeros((x,y,z,self.channels*3),device=tensor.device).type(tensor.type())\n",
    "        emb[:,:,:,:self.channels] = emb_x\n",
    "        emb[:,:,:,self.channels:2*self.channels] = emb_y\n",
    "        emb[:,:,:,2*self.channels:] = emb_z\n",
    "        out = emb[None,:,:,:,:orig_ch].repeat(batch_size, 1, 1, 1, 1)\n",
    "        out = out.permute(0, 3, 4, 2, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttention(nn.Module):\n",
    "    \"\"\"Sparse Self Attention Module\"\"\"\n",
    "    def __init__(self, in_dim=512):\n",
    "        \"\"\"The only iuput attribute is dimension\"\"\"\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//4, kernel_size=1)\n",
    "        self.key_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim//4, kernel_size=1)\n",
    "        self.value_conv = nn.Conv3d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=4)\n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.affinity = torch.zeros(0)\n",
    "        self.obj_affinity = torch.zeros(0)\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        # Input tensor shape: [batch_size, T, channel, H, W] \n",
    "        # Permute x to : [batch_size, channel, H, W, T]\n",
    "        x = x.permute(0, 2, 3, 4, 1)\n",
    "        b, C, H, W, T = x.shape\n",
    "        proj_query = self.query_conv(x)\n",
    "        proj_query_H = proj_query.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H).permute(0,2,1) # [b*W*T,H,C]\n",
    "        proj_query_W = proj_query.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W).permute(0,2,1) # [b*H*T,W,C]\n",
    "        proj_query_T = proj_query.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T).permute(0,2,1) # [b*W*H,T,C]\n",
    "        m_H = m.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H).permute(0,2,1) # [b*W*T,H,C]\n",
    "        m_W = m.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W).permute(0,2,1) # [b*H*T,W,C]\n",
    "        m_T = m.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T).permute(0,2,1) # [b*W*H,T,C]\n",
    "\n",
    "        proj_key = self.key_conv(x)\n",
    "        proj_key_H = proj_key.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\n",
    "        proj_key_W = proj_key.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\n",
    "        proj_key_T = proj_key.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\n",
    "\n",
    "        proj_value = self.value_conv(x)\n",
    "        proj_value_H = proj_value.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H)      # [b*W*T,C,H]\n",
    "        proj_value_W = proj_value.permute(0,2,4,1,3).contiguous().view(b*H*T,-1,W)      # [b*H*T,C,W]\n",
    "        proj_value_T = proj_value.permute(0,2,3,1,4).contiguous().view(b*W*H,-1,T)      # [b*W*H,C,T]\n",
    "\n",
    "        energy_H = torch.bmm(proj_query_H, proj_key_H).view(b,W,T,H,H).permute(0,3,1,2,4) # [b,H,W,T,H]\n",
    "        energy_W = torch.bmm(proj_query_W, proj_key_W).view(b,H,T,W,W).permute(0,1,3,2,4) # [b,H,W,T,W]\n",
    "        energy_T = torch.bmm(proj_query_T, proj_key_T).view(b,H,W,T,T)                    # [b,H,W,T,T]\n",
    "        score = self.softmax(torch.cat([energy_H,energy_W,energy_T],4))         # [b,H,W,T,(H+W+T)]\n",
    "        self.score = score.permute(0,4,3,1,2)  # [b,(H+W+T),T,H,W]\n",
    "        \n",
    "        att_H = score[:,:,:,:,0:H].permute(0,2,3,1,4).contiguous().view(b*W*T,H,H)      # [b*W*T,H,H]\n",
    "        att_W = score[:,:,:,:,H:H+W].permute(0,1,4,2,3).contiguous().view(b*H*T,W,W)    # [b*H*T,W,W]\n",
    "        att_T = score[:,:,:,:,H+W:].contiguous().view(b*H*W,T,T)                        # [b*H*W,T,T]\n",
    "\n",
    "        out_H = torch.bmm(proj_value_H, att_H.permute(0,2,1)).view(b,W,T,-1,H).permute(0,3,4,1,2)\n",
    "        out_W = torch.bmm(proj_value_W, att_W.permute(0,2,1)).view(b,H,T,-1,W).permute(0,3,1,4,2)\n",
    "        out_T = torch.bmm(proj_value_T, att_T.permute(0,2,1)).view(b,H,W,-1,T).permute(0,3,1,2,4)\n",
    "\n",
    "        # objaff_H = torch.bmm(m_H, att_H.permute(0,2,1)).view(b,W,T,-1,H).permute(0,3,4,1,2)\n",
    "        # objaff_W = torch.bmm(m_H, att_W.permute(0,2,1)).view(b,H,T,-1,W).permute(0,3,1,4,2)\n",
    "        # objaff_T = torch.bmm(m_H, att_T.permute(0,2,1)).view(b,H,W,-1,T).permute(0,3,1,2,4)\n",
    "        objaff_H = torch.mul(att_H, m_H).view(b,W,T,-1,H).permute(0,3,4,1,2)\n",
    "        objaff_W = torch.mul(att_W, m_W).view(b,H,T,-1,W).permute(0,3,1,4,2)\n",
    "        objaff_T = torch.mul(att_T, m_T).view(b,H,W,-1,T).permute(0,3,1,2,4)\n",
    "        \n",
    "        # permute back to [batch_size, T, channel, H, W] \n",
    "        output = self.gamma*(out_H + out_T + out_W).permute(0,4,1,2,3)\n",
    "        objaff = torch.cat([objaff_H, objaff_W, objaff_T], 1)\n",
    "        objaff = torch.max(objaff, dim=1).values.permute(0,3,1,2)\n",
    "        self.obj_affinity = objaff\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 32, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = b0.permute(0,2,3,4,1)\n",
    "b, C, H, W, T = m.shape\n",
    "m = m.permute(0,3,4,1,2).contiguous().view(b*W*T,-1,H).permute(0,2,1)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-275f0f00cde1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mSA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparseAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mobjaff_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjaff_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjaff_T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mobjaff_H\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjaff_W\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjaff_T\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "CNN = CNNBackbone()\n",
    "SA = SparseAttention()\n",
    "z = CNN(a)\n",
    "objaff_H, objaff_W, objaff_T = SA(z, b0)\n",
    "objaff_H.shape, objaff_W.shape, objaff_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTEncoder(nn.Module):\n",
    "    \"\"\"Define the Multi-head attention -> Add&Norm -> Feed Forward -> Add&Norm module\"\"\"\n",
    "    def __init__(self, dim=512, dropout=0.2):\n",
    "        super(SSTEncoder, self).__init__()\n",
    "\n",
    "        # Multi-head attention sub-layer\n",
    "        self.attn = SparseAttention(dim)\n",
    "        self.norm_1 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Feed forward sub-layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=dim*2, out_features=dim)\n",
    "        )\n",
    "        self.norm_2 =  nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x, m):\n",
    "        y1 = self.attn(x, m)\n",
    "        x2 = (x+y1).permute(0,3,4,1,2)   # permute from [b,T,C,H,W] to [b,H,W,T,C]\n",
    "        y2 = self.norm_1(x2)\n",
    "        y3 = self.fc(y2)\n",
    "        out = self.norm_2(y2+y3).permute(0,3,4,1,2)  # permute from [b,H,W,T,C] to [b,T,C,H,W]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Define Decoder block for deconvolution\"\"\"\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, deconv=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # self.in_channels = in_channels\n",
    "        if deconv:\n",
    "            self.Deblock = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(mid_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(mid_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            self.Deblock = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinar'),\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "        y = self.Deblock(x)\n",
    "        y = y.unsqueeze(0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, num_layers=4):\n",
    "        super(MyTransformer, self).__init__()\n",
    "\n",
    "        self.backbone = CNNBackbone()\n",
    "        self.pos_encoding = PositionalEncoding3D()\n",
    "        self.self_attn1 = SSTEncoder()\n",
    "        self.self_attn2 = SSTEncoder()\n",
    "        self.self_attn3 = SSTEncoder()\n",
    "        self.self_attn4 = SSTEncoder()\n",
    "        # self.dec1 = DecoderBlock(512*2+4,512,512)\n",
    "        # input dim = CNN_out * 2 + num_layers * N_classes\n",
    "        self.dec1 = DecoderBlock(512*2+4*1,512,512)\n",
    "        self.dec2 = DecoderBlock(512,128,128)\n",
    "        self.dec3 = DecoderBlock(128,64,1)\n",
    "        # self.dec4 = DecoderBlock(128,64,8)\n",
    "        self.cnn_feat = torch.zeros(1)\n",
    "        self.encod_feat = torch.zeros(1)\n",
    "        self.attn_scores = torch.zeros(1)\n",
    "        self.object_affinity = torch.zeros(1)\n",
    "    \n",
    "\n",
    "    def save_preds(self, z, loc=\"\"):\n",
    "        \"\"\"Save the predicted mask to file.\"\"\"\n",
    "        path = loc.replace(\"images\",\"prediction_temp\").replace(\".jpg\",\".pt\")\n",
    "        z = F.interpolate(z.squeeze(0).float(), size=(32,40), mode='bilinear', align_corners=True)\n",
    "        preds = torch.sigmoid(z[-1]) > 0.5\n",
    "        preds = preds.to(torch.float32).cpu()\n",
    "        torch.save(preds, path)\n",
    "\n",
    "\n",
    "    def forward(self, x, m, loc=\"\"):\n",
    "        \"\"\"\n",
    "        x - input frames sequence;\n",
    "        loc - path of the last frame\n",
    "        \"\"\"\n",
    "        # 1. subtract feature embedding from CNN backbone [b,T,C,H',W']\n",
    "        cnn_fs = self.backbone(x)\n",
    "        self.cnn_feat = cnn_fs\n",
    "        # Get positional encoding and add it to the feature embedding\n",
    "        pe = self.pos_encoding(cnn_fs)\n",
    "        y = pe + cnn_fs\n",
    "\n",
    "        # 2. Encoded feature from transformer layers \n",
    "        y = self.self_attn1(y, m)\n",
    "        y = self.self_attn2(y, m)\n",
    "        y = self.self_attn3(y, m)\n",
    "        trans_feat = self.self_attn4(y, m)\n",
    "        self.encod_feat = trans_feat\n",
    "        \n",
    "        # # 3. Object Affinity Value [N_layers,Obj,T,H',W']:\n",
    "        affinity1 = self.self_attn1.attn.obj_affinity\n",
    "        affinity2 = self.self_attn2.attn.obj_affinity\n",
    "        affinity3 = self.self_attn3.attn.obj_affinity\n",
    "        affinity4 = self.self_attn4.attn.obj_affinity\n",
    "        object_affinity = torch.stack([affinity1, affinity2, affinity3, affinity4], 0)\n",
    "        object_affinity = object_affinity.flatten(0,1).transpose(0,1).unsqueeze(0)\n",
    "        self.object_affinity = object_affinity\n",
    "        # # Adjust to [b,T,C,H',W']\n",
    "\n",
    "        # Concatnate feature together\n",
    "        emb = torch.cat([cnn_fs, trans_feat, object_affinity],dim=2)\n",
    "        \n",
    "        z = self.dec1(emb)\n",
    "        z = self.dec2(z)\n",
    "        z = self.dec3(z)\n",
    "        # z = self.dec4(z)\n",
    "        self.save_preds(z, loc)\n",
    "        output = F.interpolate(z[-1],scale_factor=4,mode='bilinear',align_corners=True)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1024, 1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyTransformer()\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "z = model(a.to(device), b0.to(device), c[0])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 1024, 1280]), torch.Size([5, 1, 1024, 1280]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data, mask, temp, path = next(iter(training_data_loader))\n",
    "# data, mask, temp, path = data.to(device), mask.float().to(device), temp.to(device), path[0]\n",
    "# output = model(data, temp, path)                            # Forward Passing\n",
    "# target = mask.transpose(0,1)\n",
    "# # loss = criterion(output, target)                  # Compute loss\n",
    "# preds = torch.sigmoid(output) > 0.5            # Make prediction\n",
    "# preds = preds.to(torch.float32) \n",
    "# # loss.backward()                                 # Compute gradients\n",
    "# # optimizer.step()                                # Update the model parameters\n",
    "# # optimizer.zero_grad()                           # Clear the gradients\n",
    "# # train_loss += loss.item() * data.size(0)        # Compute training loss\n",
    "# # train_Jaccard += get_jaccard(target, preds)\n",
    "# preds.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard(y_true, y_pred):\n",
    "    intersection = (y_true * y_pred).sum()\n",
    "    union = y_true.sum() + y_pred.sum() - intersection\n",
    "    res = (intersection + 1e-15) / (union + 1e-15)\n",
    "    return res.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def get_Dice(y_true, y_pred):\n",
    "    res = (2 * (y_true * y_pred).sum() + 1e-15) / (y_true.sum() + y_pred.sum() + 1e-15)\n",
    "    return res.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, criterion, optimizer, epochs):\n",
    "    # Initialize lists to store loss and fscore each epoch\n",
    "    LOSS_train = []\n",
    "    LOSS_valid = []\n",
    "    Jaccard_train = []\n",
    "    Jaccard_valid = []  \n",
    "    Dice_train = []\n",
    "    Dice_valid = []\n",
    "\n",
    "    for epoch in tqdm.trange(epochs, desc=\"Epochs\"):\n",
    "        result = []\n",
    "        train_loss = 0.0\n",
    "        train_Jaccard = 0.0\n",
    "        train_dice = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_Jaccard = 0.0\n",
    "        valid_dice = 0.0\n",
    "\n",
    "        # Begin training\n",
    "        model.train()\n",
    "        for data, mask, temp, path in train_dataloader:\n",
    "            data, mask, temp, path = data.to(device), mask.float().to(device), temp.to(device), path[0]\n",
    "            output = model(data, temp, path)                            # Forward Passing\n",
    "            target = mask.transpose(0,1)\n",
    "            loss = criterion(output, target)                  # Compute loss\n",
    "            preds = torch.sigmoid(output) > 0.5            # Make prediction\n",
    "            preds = preds.to(torch.float32) \n",
    "            loss.backward()                                 # Compute gradients\n",
    "            optimizer.step()                                # Update the model parameters\n",
    "            optimizer.zero_grad()                           # Clear the gradients\n",
    "            train_loss += loss.item() * data.size(0)        # Compute training loss\n",
    "            train_Jaccard += get_jaccard(target[-1][0], preds[-1][0])\n",
    "            train_dice += get_Dice(target[-1][0], preds[-1][0])\n",
    "\n",
    "        # Begin validation\n",
    "        model.eval()\n",
    "        for data, mask, temp, path in valid_dataloader:\n",
    "            data, mask, temp, path = data.to(device), mask.float().to(device), temp.to(device), path[0]\n",
    "            output = model(data, temp, path)                             # Forward Passing\n",
    "            target = mask.transpose(0,1)\n",
    "            loss = criterion(output, target)                  # Compute loss\n",
    "            preds = torch.sigmoid(output) > 0.5            # Make prediction\n",
    "            preds = preds.to(torch.float32) \n",
    "            valid_loss += loss.item() * data.size(0)        # Compute validation loss\n",
    "            valid_Jaccard += get_jaccard(target[-1][0], preds[-1][0])\n",
    "            valid_dice += get_Dice(target[-1][0], preds[-1][0])\n",
    "        \n",
    "        # Compute epoch loss and f1\n",
    "        epoch_train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        epoch_train_Jaccard = train_Jaccard / len(train_dataloader.dataset)\n",
    "        epoch_train_dice = train_dice / len(train_dataloader.dataset)\n",
    "        epoch_valid_loss = valid_loss / len(valid_dataloader.dataset)\n",
    "        epoch_valid_Jaccard = valid_Jaccard / len(valid_dataloader.dataset)\n",
    "        epoch_valid_dice = valid_dice / len(valid_dataloader.dataset)\n",
    "\n",
    "        # Record epoch loss and f1 to the list\n",
    "        LOSS_train.append(epoch_train_loss)\n",
    "        LOSS_valid.append(epoch_valid_loss)\n",
    "        Jaccard_train.append(epoch_train_Jaccard)\n",
    "        Jaccard_valid.append(epoch_valid_Jaccard)   \n",
    "        Dice_train.append(epoch_train_dice)\n",
    "        Dice_valid.append(epoch_valid_dice)\n",
    "\n",
    "        result.append(f'{epoch} TRAIN loss: {epoch_train_loss:.4f}, Jaccard: {epoch_train_Jaccard:.4f}, Dice: {epoch_train_dice:.4f};  VALID loss: {epoch_valid_loss:.4f}, Jaccard: {epoch_valid_Jaccard:.4f}, Dice: {epoch_valid_dice:.4f}')\n",
    "\n",
    "        print(result)\n",
    "    return LOSS_train, LOSS_valid, Jaccard_train, Jaccard_valid, Dice_train, Dice_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 1/3 [07:52<15:44, 472.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 TRAIN loss: 0.0837, Jaccard: 0.7853, Dice: 0.8638;  VALID loss: 0.2267, Jaccard: 0.6864, Dice: 0.7963']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 2/3 [15:55<07:58, 478.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 TRAIN loss: 0.0734, Jaccard: 0.7946, Dice: 0.8711;  VALID loss: 0.2171, Jaccard: 0.6919, Dice: 0.7996']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 3/3 [23:59<00:00, 479.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 TRAIN loss: 0.0691, Jaccard: 0.8001, Dice: 0.8753;  VALID loss: 0.2137, Jaccard: 0.6931, Dice: 0.8003']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-d5e722e5574e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# criterion = nn.BCEWithLogitsLoss()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mLOSS_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOSS_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFSCORE_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFSCORE_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "model = MyTransformer()\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "LOSS_train, LOSS_valid, FSCORE_train, FSCORE_valid = train(model, training_data_loader, valid_data_loader, criterion, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1, 5, 3, 256, 320])\n",
      "Mask shape: torch.Size([1, 5, 1024, 1280])\n",
      "Path: C:\\Users\\Siyao\\Downloads\\EndoVis2017Data\\cropped_train\\instrument_dataset_1\\images\\frame224.jpg\n"
     ]
    }
   ],
   "source": [
    "frame,mask,pred,c = valid_data_frames[220]\n",
    "# img = a[0][0].permute(1,2,0)\n",
    "# plt.imshow(img)\n",
    "frame = frame.unsqueeze(0)\n",
    "mask = mask.unsqueeze(0)\n",
    "pred = pred.unsqueeze(0)\n",
    "print(f\"Data shape: {frame.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Path: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img, pred, c):\n",
    "    # with torch.no_grad():\n",
    "    model.eval()\n",
    "    img = img.to(device)\n",
    "    pred = pred.to(device)\n",
    "    prd = model(img, pred, c[0])\n",
    "    prd = torch.sigmoid(prd) > 0.5\n",
    "    prd = prd.float()\n",
    "    return prd.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1024, 1280])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd = get_mask(frame, pred, c)\n",
    "prd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aab94ec4c997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(frame[0][].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25234ec8df0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x252375c19a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(prd[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23d8489b17aa989cee42bd3f3e82cc035b4cfa42fa1ea343caf5051171f57614"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('datasci': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
