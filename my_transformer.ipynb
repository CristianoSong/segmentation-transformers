{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Toy Transformer \r\n",
    "This file has been created to understand the attention/transformer structure and basic method to build transformer in PyTorch.  \""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Preparation - package import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import torch\r\n",
    "import random\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import torchvision\r\n",
    "from pathlib import Path\r\n",
    "from torch import nn\r\n",
    "import torch.utils.data as Data\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.nn.utils.rnn import pad_sequence\r\n",
    "from torch.nn import functional as F\r\n",
    "from torchvision import models\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import matplotlib\r\n",
    "\r\n",
    "%matplotlib inline\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset and Dataloader\r\n",
    "### Train / valid split based on the **folds** argument"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_path = Path(\"C:/Users/Siyao/Downloads/EndoVis2017Data\")\r\n",
    "train_path = data_path / \"cropped_train\"\r\n",
    "\r\n",
    "def get_split(fold):\r\n",
    "    \"\"\"Split train and valid dataset based on the No. of folder\"\"\"\r\n",
    "    folds = {0: [1, 3],\r\n",
    "             1: [2, 5],\r\n",
    "             2: [4, 8],\r\n",
    "             3: [6, 7]}\r\n",
    "    train_path = data_path / 'cropped_train'\r\n",
    "\r\n",
    "    train_file_names = []\r\n",
    "    val_file_names = []\r\n",
    "\r\n",
    "    for instrument_id in range(1, 9):\r\n",
    "        if instrument_id in folds[fold]:\r\n",
    "            val_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\r\n",
    "        else:\r\n",
    "            train_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\r\n",
    "\r\n",
    "    return train_file_names, val_file_names\r\n",
    "\r\n",
    "train_file_names, val_file_names = get_split(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to load image or mask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def load_image(path):\r\n",
    "    img = cv2.imread(str(path))\r\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
    "\r\n",
    "binary_factor = 255\r\n",
    "parts_factor = 85\r\n",
    "instrument_factor = 32\r\n",
    "\r\n",
    "def load_mask(path, problem_type=\"instruments\", mask_folder=\"instruments_masks\",factor=instrument_factor):\r\n",
    "    if problem_type == 'binary':\r\n",
    "        mask_folder = 'binary_masks'\r\n",
    "        factor = binary_factor\r\n",
    "    elif problem_type == 'parts':\r\n",
    "        mask_folder = 'parts_masks'\r\n",
    "        factor = parts_factor\r\n",
    "    elif problem_type == 'instruments':\r\n",
    "        factor = instrument_factor\r\n",
    "        mask_folder = 'instruments_masks'\r\n",
    "\r\n",
    "    mask = cv2.imread(str(path).replace('images', mask_folder).replace('jpg', 'png'), 0)\r\n",
    "\r\n",
    "    return (mask / factor).astype(np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset for training and validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class RoboticsDataset(Dataset):\r\n",
    "    \"\"\"Dataset that only loads single frame\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, file_names, to_augment=False, transform=None, mode='train', problem_type=None):\r\n",
    "        self.file_names = file_names\r\n",
    "        self.to_augment = to_augment\r\n",
    "        self.transform = transform\r\n",
    "        self.mode = mode\r\n",
    "        self.problem_type = problem_type\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.file_names)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img_file_name = self.file_names[idx]\r\n",
    "        image = load_image(img_file_name)\r\n",
    "        mask = load_mask(img_file_name, self.problem_type)\r\n",
    "\r\n",
    "        # data = {\"image\": image, \"mask\": mask}\r\n",
    "        # augmented = self.transform(**data)\r\n",
    "        # image, mask = augmented[\"image\"], augmented[\"mask\"]\r\n",
    "\r\n",
    "        # if self.mode == 'train':\r\n",
    "        if self.problem_type == 'binary':\r\n",
    "            return torch.from_numpy(image), torch.from_numpy(np.expand_dims(mask, 0)).float(), str(img_file_name)\r\n",
    "        else:\r\n",
    "            return torch.from_numpy(image), torch.from_numpy(mask).long(), str(img_file_name)\r\n",
    "        # else:\r\n",
    "        #     return torch.from_numpy(image), str(img_file_name)\r\n",
    "\r\n",
    "train_data_single = RoboticsDataset(train_file_names, problem_type=\"instrument\")\r\n",
    "valid_data_single = RoboticsDataset(val_file_names, mode='valid')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instrument Dataset\r\n",
    "1. Mutiple image stacked as data\r\n",
    "2. The label used instrument type"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Creating the lists of file name, that starts from tau frames after the first frame\r\n",
    "### which avoid the first few frames having no previous frames issue.\r\n",
    "\r\n",
    "tau = 3\r\n",
    "train_img_path = [str(i) for i in train_file_names]\r\n",
    "train_frame_name = [i for i in train_img_path if int(i[-7:-4])>=tau]\r\n",
    "valid_img_path = [str(i) for i in val_file_names] \r\n",
    "valid_frame_name = [i for i in valid_img_path if int(i[-7:-4])>=tau]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class InstrumentDataset(Dataset):\r\n",
    "    \"\"\"Dataset that loads multiple frame\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, file_names, problem_type=\"Instrument\", tau=3):\r\n",
    "        self.file_names = file_names\r\n",
    "        self.problem_type = problem_type\r\n",
    "        self.tau = tau      # tau is the number of frames should be combiend\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.file_names)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        current_frame = self.file_names[idx]\r\n",
    "        mask = load_mask(current_frame, self.problem_type)\r\n",
    "        frames_ls = []\r\n",
    "        for i in range(tau):\r\n",
    "            to_find = \"frame\"+current_frame[-7:-4]\r\n",
    "            to_repl = \"frame\"+ '%03d' % (int(current_frame[-7:-4])-i)\r\n",
    "            frame = current_frame.replace(to_find, to_repl)\r\n",
    "            frame_array = load_image(frame)\r\n",
    "            frame_tensor = torch.from_numpy(frame_array)\r\n",
    "            frames_ls.append(frame_tensor)\r\n",
    "        frames_stack = torch.stack(frames_ls, 0)\r\n",
    "        # permute the tensor from [tau, H, W, C] to [tau, C, H, W]\r\n",
    "        frames_tensor = frames_stack.permute(0,3,1,2)\r\n",
    "        return frames_tensor.float(), torch.from_numpy(mask).float(), str(current_frame) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### Traning and validation data with multi-frames input\r\n",
    "# The data in 4D tensor (tau, w, h, c), label in 3D tensor ()\r\n",
    "training_data_frames = InstrumentDataset(train_frame_name)\r\n",
    "valid_data_frames = InstrumentDataset(valid_frame_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "batch_size = 1\r\n",
    "training_data_loader = Data.DataLoader(training_data_frames, batch_size=batch_size, shuffle=True)\r\n",
    "valid_data_loader = Data.DataLoader(valid_data_frames, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "a,b,c = next(iter(training_data_loader))\r\n",
    "a.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 1024, 1280])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\r\n",
    "### CNN Backbone"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = models.resnet101(pretrained=True) # load the pretrained model\r\n",
    "for param in model.parameters(): param.requires_grad_(False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model = models.resnet101(pretrained=True)\r\n",
    "for param in model.parameters():\r\n",
    "    param.requires_grad_(False)\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "model.to(device)\r\n",
    "print(\"finish\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finish\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNNBackbone(nn.Modules):\r\n",
    "    def __init__(self, c_in, n_out):\r\n",
    "        super(CNNBackbone, self).__init__()\r\n",
    "        self.c_in = c_in\r\n",
    "        self.n_out = n_out\r\n",
    "\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\r\n",
    "    \"Implement the PE function.\"\r\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n",
    "        super(PositionalEncoding, self).__init__()\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "        \r\n",
    "        # Compute the positional encodings once in log space.\r\n",
    "        pe = torch.zeros(max_len, d_model)\r\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\r\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\r\n",
    "                             -(math.log(10000.0) / d_model))\r\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
    "        pe = pe.unsqueeze(0)\r\n",
    "        self.register_buffer('pe', pe)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \r\n",
    "                         requires_grad=False)\r\n",
    "        return self.dropout(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MyTransSeg(nn.Module):\r\n",
    "    \"\"\"Use a CNN to extract information from each frame\"\"\"\r\n",
    "    def __init__(self, c_in, n_classes, use_gt=True):\r\n",
    "        super(MyTransSeg, self).__init__():\r\n",
    "        self.c_in = c_in\r\n",
    "        self.n_classes = n_classes\r\n",
    "        self.use_gt = use_gt\r\n",
    "        self.backbone = models.resnet101(pretrained=True)\r\n",
    "        self.backbone.eval()\r\n",
    "        self.postionencoding = PositionalEncoding()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def clones(module, N):\r\n",
    "    \"Produce N identical layers; stack N modules.\"\r\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "a[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 1024, 1280])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = a.permute(0,1,4,2,3)\r\n",
    "a.shape\r\n",
    "a.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 1024, 1280])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "a.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1280, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Positional Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\r\n",
    "    \"Implement the PE function.\"\r\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\r\n",
    "        super(PositionalEncoding, self).__init__()\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "        \r\n",
    "        # Compute the positional encodings once in log space.\r\n",
    "        pe = torch.zeros(max_len, d_model)\r\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\r\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\r\n",
    "                             -(math.log(10000.0) / d_model))\r\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
    "        pe = pe.unsqueeze(0)\r\n",
    "        self.register_buffer('pe', pe)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \r\n",
    "                         requires_grad=False)\r\n",
    "        return self.dropout(x)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('datasci': conda)"
  },
  "interpreter": {
   "hash": "23d8489b17aa989cee42bd3f3e82cc035b4cfa42fa1ea343caf5051171f57614"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}